{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Phase 1: Foundations & Data Pipeline\n", "\n", "## Overview\n", "This notebook demonstrates Phase 1 of our Contract Analysis System:\n", "- CUAD dataset ingestion and processing\n", "- Contract metadata extraction using NER\n", "- Clause segmentation and classification\n", "- Baseline model training (TF-IDF + Logistic Regression)\n", "- Simple risk scoring with keyword patterns\n", "- Data validation with Pandera\n", "\n", "**Author**: Mohammad Babaie\n", "**Email**: mj.babaie@gmail.com\n", "**LinkedIn**: https://www.linkedin.com/in/mohammadbabaie/\n", "**GitHub**: https://github.com/Muh76"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import required libraries\n", "import sys\n", "import os\n", "import json\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from pathlib import Path\n", "from datetime import datetime\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "\n", "# Add src to path for imports\n", "sys.path.append(\"src\")\n", "\n", "# Import our custom modules\n", "from data.pipeline import ContractDataPipeline, ContractMetadata, ClauseSegment\n", "from models.baseline_models import BaselineClauseClassifier, KeywordRiskScorer, BaselineEvaluator\n", "\n", "# Set up plotting style\n", "plt.style.use(\"seaborn-v0_8\")\n", "sns.set_palette(\"husl\")\n", "\n", "print(\"‚úÖ All libraries imported successfully!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Data Pipeline Demonstration\n", "\n", "### 1.1 Initialize Pipeline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Configuration\n", "config = {\n", "    \"cuad_file_path\": \"data/raw/CUAD_v1.json\",\n", "    \"output_dir\": \"data/processed\",\n", "    \"validation_enabled\": True,\n", "    \"logging_level\": \"INFO\"\n", "}\n", "\n", "# Initialize pipeline\n", "pipeline = ContractDataPipeline(config)\n", "print(\"‚úÖ Pipeline initialized successfully!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.2 Load CUAD Dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load CUAD dataset\n", "print(\"üìä Loading CUAD dataset...\")\n", "cuad_df = pipeline.load_cuad_dataset(config[\"cuad_file_path\"])\n", "\n", "print(f\"‚úÖ Loaded {len(cuad_df)} contracts from CUAD dataset\")\n", "print(f\"\nDataset columns: {list(cuad_df.columns)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.3 Explore Sample Contract"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Explore a sample contract\n", "sample_contract = cuad_df.iloc[0]\n", "print(\"üìÑ Sample Contract Analysis\")\n", "print(\"=\" * 50)\n", "print(f\"Contract ID: {sample_contract[\"contract_id\"]}\")\n", "print(f\"Title: {sample_contract[\"title\"]}\")\n", "print(f\"Text length: {len(sample_contract[\"context\"])} characters\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.4 Process Contract with Metadata Extraction"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Process a sample contract\n", "print(\"üîç Processing sample contract...\")\n", "metadata, clauses = pipeline.process_contract(\n", "    sample_contract[\"context\"], \n", "    sample_contract[\"contract_id\"]\n", ")\n", "\n", "print(f\"\nüìã Extracted Metadata:\")\n", "print(f\"  Contract Type: {metadata.contract_type}\")\n", "print(f\"  Parties: {metadata.parties}\")\n", "print(f\"  Total Clauses: {metadata.total_clauses}\)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Data Analysis & Visualization\n", "\n", "### 2.1 Process Multiple Contracts"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Process multiple contracts for analysis\n", "print(\"üîÑ Processing multiple contracts for analysis...\")\n", "all_metadata = []\n", "all_clauses = []\n", "\n", "# Process first 10 contracts for demonstration\n", "for idx, contract in cuad_df.head(10).iterrows():\n", "    try:\n", "        metadata, clauses = pipeline.process_contract(\n", "            contract[\"context\"], \n", "            contract[\"contract_id\"]\n", "        )\n", "        all_metadata.append(metadata)\n", "        all_clauses.append(clauses)\n", "    except Exception as e:\n", "        print(f\"Error processing contract {contract[\"contract_id\"]}: {e}\")\n", "        continue\n", "\n", "print(f\"‚úÖ Processed {len(all_metadata)} contracts successfully!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2 Generate Data Report"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Generate data report\n", "report = pipeline.generate_data_report(all_metadata, all_clauses)\n", "\n", "print(\"üìä Data Analysis Report\")\n", "print(\"=\" * 50)\n", "print(f\"Total Contracts: {report[\"summary\"][\"total_contracts\"]}\")\n", "print(f\"Total Clauses: {report[\"summary\"][\"total_clauses\"]}\")\n", "print(f\"Avg Clauses per Contract: {report[\"summary\"][\"avg_clauses_per_contract\"]:.2f}\)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.3 Visualize Data Distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize contract and clause types\n", "plt.figure(figsize=(15, 5))\n", "\n", "# Contract types\n", "plt.subplot(1, 3, 1)\n", "contract_types = list(report[\"contract_types\"].keys())\n", "contract_counts = list(report[\"contract_types\"].values())\n", "plt.pie(contract_counts, labels=contract_types, autopct=\"%1.1f%%\")\n", "plt.title(\"Contract Type Distribution\")\n", "\n", "# Clause types\n", "plt.subplot(1, 3, 2)\n", "clause_types = list(report[\"clause_types\"].keys())\n", "clause_counts = list(report[\"clause_types\"].values())\n", "plt.bar(range(len(clause_types)), clause_counts)\n", "plt.xticks(range(len(clause_types)), clause_types, rotation=45, ha=\"right\")\n", "plt.title(\"Clause Type Distribution\")\n", "plt.ylabel(\"Count\")\n", "\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Baseline Model Training & Evaluation\n", "\n", "### 3.1 Prepare Data for Models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Prepare data for baseline models\n", "print(\"üîß Preparing data for baseline models...\")\n", "\n", "# Create DataFrame from processed clauses\n", "clauses_data = []\n", "for clauses in all_clauses:\n", "    for clause in clauses:\n", "        clauses_data.append({\n", "            \"text\": clause.text,\n", "            \"clause_type\": clause.clause_type,\n", "            \"confidence\": clause.confidence,\n", "            \"risk_flags\": clause.risk_flags\n", "        })\n", "\n", "clauses_df = pd.DataFrame(clauses_data)\n", "print(f\"‚úÖ Prepared {len(clauses_df)} clauses for training\")\n", "print(f\"Clause types: {clauses_df[\"clause_type\"].value_counts().to_dict()}\)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2 Initialize Baseline Models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize baseline models\n", "model_config = {\n", "    \"data_path\": \"data/processed/clause_segments.csv\",\n", "    \"output_dir\": \"models/baseline\",\n", "    \"test_size\": 0.2,\n", "    \"random_state\": 42\n", "}\n", "\n", "classifier = BaselineClauseClassifier(model_config)\n", "risk_scorer = KeywordRiskScorer()\n", "evaluator = BaselineEvaluator(model_config)\n", "\n", "print(\"‚úÖ Baseline models initialized!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3 Train Baseline Classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Prepare data for classification\n", "X, y = classifier.prepare_data(clauses_df)\n", "print(f\"‚úÖ Data prepared: {X.shape[0]} samples, {X.shape[1]} features\")\n", "print(f\"Clause types: {list(classifier.clause_types)}\)\n", "\n", "# Split data and train model\n", "from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    X, y, test_size=0.2, random_state=42, stratify=y\n", ")\n", "\n", "print(\"üöÄ Training baseline classifier...\")\n", "classifier_metrics = classifier.train_model(X_train, y_train, model_type=\"logistic\")\n", "\n", "print(f\"\nüìà Training Results:\")\n", "print(f\"  Model Type: {classifier_metrics[\"model_type\"]}\")\n", "print(f\"  CV F1 Macro: {classifier_metrics[\"cv_f1_macro_mean\"]:.3f} (+/- {classifier_metrics[\"cv_f1_macro_std\"]*2:.3f})\")\n", "print(f\"  Training F1 Macro: {classifier_metrics[\"f1_macro\"]:.3f}\)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.4 Evaluate Model Performance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Evaluate classifier\n", "print(\"üîç Evaluating classifier...\")\n", "classifier_results = evaluator.evaluate_classifier(classifier, X_test, y_test)\n", "\n", "print(f\"\nüìä Test Results:\")\n", "print(f\"  Test F1 Macro: {classifier_results[\"f1_macro\"]:.3f}\")\n", "print(f\"  Test F1 Weighted: {classifier_results[\"f1_weighted\"]:.3f}\")\n", "print(f\"  Test Samples: {classifier_results[\"test_samples\"]}\")\n", "print(f\"  Prediction Confidence: {classifier_results[\"prediction_confidence\"]:.3f}\)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.5 Visualize Model Performance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize model performance\n", "plt.figure(figsize=(15, 5))\n", "\n", "# Confusion matrix\n", "plt.subplot(1, 3, 1)\n", "conf_matrix = np.array(classifier_results[\"confusion_matrix\"])\n", "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n", "            xticklabels=classifier.clause_types, \n", "            yticklabels=classifier.clause_types)\n", "plt.title(\"Confusion Matrix\")\n", "plt.xlabel(\"Predicted\")\n", "plt.ylabel(\"Actual\")\n", "\n", "# Per-class F1 scores\n", "plt.subplot(1, 3, 2)\n", "f1_scores = list(classifier_results[\"f1_per_class\"].values())\n", "clause_types = list(classifier_results[\"f1_per_class\"].keys())\n", "plt.bar(range(len(clause_types)), f1_scores)\n", "plt.xticks(range(len(clause_types)), clause_types, rotation=45, ha=\"right\")\n", "plt.title(\"Per-Class F1 Scores\")\n", "plt.ylabel(\"F1 Score\")\n", "\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Risk Scoring Analysis\n", "\n", "### 4.1 Evaluate Risk Scorer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Evaluate risk scorer\n", "print(\"üîç Evaluating risk scorer...\")\n", "test_clauses = clauses_df[\"text\"].tolist()\n", "risk_results = evaluator.evaluate_risk_scorer(risk_scorer, test_clauses)\n", "\n", "print(f\"\nüìä Risk Analysis Results:\")\n", "print(f\"  Average Risk Score: {risk_results[\"avg_risk_score\"]:.3f}\")\n", "print(f\"  Total Clauses: {risk_results[\"total_clauses\"]}\")\n", "print(f\"  Clauses with Risks: {risk_results[\"clauses_with_risks\"]}\)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.2 Demonstrate Risk Scoring"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Demonstrate risk scoring on sample clauses\n", "print(\"üîç Risk Scoring Examples\")\n", "print(\"=\" * 50)\n", "\n", "sample_clauses = [\n", "    \"The party shall have unlimited liability for all damages.\",\n", "    \"The party shall use reasonable efforts to complete the work.\",\n", "    \"Both parties agree to standard terms and conditions.\",\n", "    \"Either party may terminate this agreement at will.\",\n", "    \"The contractor shall indemnify the client for any losses.\"\n", "]\n", "\n", "for i, clause in enumerate(sample_clauses, 1):\n", "    risk_result = risk_scorer.score_clause(clause)\n", "    print(f\"\nClause {i}: {clause}\")\n", "    print(f\"  Risk Score: {risk_result[\"risk_score\"]:.2f}/10\")\n", "    print(f\"  Risk Level: {risk_result[\"risk_level\"]}\")\n", "    print(f\"  Detected Risks: {risk_result[\"detected_risks\"]}\)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Phase 1 Summary & Next Steps\n", "\n", "### 5.1 Business Insights"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Business insights analysis\n", "print(\"üíº Phase 1 Business Insights\")\n", "print(\"=\" * 50)\n", "\n", "print(\"\nüìä Contract Analysis Summary:\")\n", "print(f\"  ‚Ä¢ Total contracts analyzed: {report[\"summary\"][\"total_contracts\"]}\")\n", "print(f\"  ‚Ä¢ Average clauses per contract: {report[\"summary\"][\"avg_clauses_per_contract\"]:.1f}\)\n", "\n", "print(f\"\n‚ö†Ô∏è Risk Analysis Summary:\")\n", "print(f\"  ‚Ä¢ Average risk score: {risk_results[\"avg_risk_score\"]:.2f}/10\")\n", "print(f\"  ‚Ä¢ Clauses with risks: {risk_results[\"clauses_with_risks\"]}\)\n", "\n", "print(f\"\nü§ñ Model Performance Summary:\")\n", "print(f\"  ‚Ä¢ Classification F1 Score: {classifier_results[\"f1_macro\"]:.3f}\")\n", "print(f\"  ‚Ä¢ Prediction confidence: {classifier_results[\"prediction_confidence\"]:.3f}\)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.2 Phase 1 Achievements"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Phase 1 achievements\n", "print(\"\nüéâ Phase 1 Achievements\")\n", "print(\"=\" * 40)\n", "\n", "achievements = [\n", "    f\"‚úÖ Data Pipeline: {report[\"summary\"][\"total_contracts\"]} contracts processed\",\n", "    f\"‚úÖ Baseline Models: F1={classifier_results[\"f1_macro\"]:.3f}\",\n", "    f\"‚úÖ Risk Analysis: {risk_results[\"clauses_with_risks\"]} clauses with risks detected\",\n", "    f\"‚úÖ Data Validation: Pandera schemas implemented\",\n", "    f\"‚úÖ NER Extraction: Metadata extraction with spaCy\",\n", "    f\"‚úÖ Reproducible Pipeline: DVC integration ready\"\n", "]\n", "\n", "for i, achievement in enumerate(achievements, 1):\n", "    print(f\"{i}. {achievement}\)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.3 Next Steps - Phase 2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Next steps\n", "print(\"\nüöÄ Ready for Phase 2: Advanced Modeling & Risk Scoring\")\n", "print(\"=\" * 60)\n", "\n", "phase2_goals = [\n", "    \"ÔøΩÔøΩ Fine-tune DistilBERT/Legal-BERT for clause classification\",\n", "    \"üéØ Implement calibrated probabilities and confidence scoring\",\n", "    \"üéØ Build advanced risk engine with policy rules + learned anomalies\",\n", "    \"üéØ Add explainability with token-level highlights + SHAP\",\n", "    \"üéØ Target macro F1 ‚â• strong baseline\",\n", "    \"üéØ Implement contract type classification\",\n", "    \"üéØ Add risk trend analysis\"\n", "]\n", "\n", "for i, goal in enumerate(phase2_goals, 1):\n", "    print(f\"{i}. {goal}\)\n", "\n", "print(f\"\nüìß Contact: mj.babaie@gmail.com\")\n", "print(f\"üîó LinkedIn: https://www.linkedin.com/in/mohammadbabaie/\")\n", "print(f\"üêô GitHub: https://github.com/Muh76\")"]}], "metadata": {"kernelspec": {"display_name": "Contract Analysis (Python 3.11)", "language": "python", "name": "contract-analysis"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.0"}}, "nbformat": 4, "nbformat_minor": 4}
